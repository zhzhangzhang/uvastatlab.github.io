<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>StatLab Articles</title>
    <link>/</link>
    <description>Recent content on StatLab Articles</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Apr 2019 21:48:51 -0700</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Getting Started with Shiny</title>
      <link>/2020/05/07/getting-started-with-shiny/</link>
      <pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/05/07/getting-started-with-shiny/</guid>
      <description>What is Shiny?Shiny is an R package that facilitates the creation of interactive web apps using R code, which can be hosted locally, on the shinyapps server, or on your own server. Shiny apps can range from extremely simple to incredibly sophisticated. They can be written purely with R code or supplemented with HTML, CSS, or JavaScript. Visit R studio’s shiny gallery to view some examples. Among its many uses, shiny apps are a great way for researchers to engage others with their work and collected data.</description>
    </item>
    
    <item>
      <title>Databases for Data Scientists</title>
      <link>/2020/04/17/databases-for-data-scientists/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/17/databases-for-data-scientists/</guid>
      <description>As data scientists, we’re often most excited about the final layer of analysis. Once all the data is cleaned and stored in a format readable by our favorite programming language (Python, R, STATA, etc), the most fun part of our work is when we’re finding counter-intuitive causations with statistical methods. If you can prove that the mutual presence of McDonalds really does prevent wars between countries or that an increase in diversity really does boost business profitability, that is a good day.</description>
    </item>
    
    <item>
      <title>Modeling Non-Constant Variance</title>
      <link>/2020/04/07/modeling-non-constant-variance/</link>
      <pubDate>Tue, 07 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/07/modeling-non-constant-variance/</guid>
      <description>One of the basic assumptions of linear modeling is constant, or homogeneous, variance. What does that mean exactly? Let’s simulate some data that satisfies this condition to illustrate the concept.
Below we create a sorted vector of numbers ranging from 1 to 10 called x, and then create a vector of numbers called y that is a function of x. When we plot x vs y, we get a straight line with an intercept of 1.</description>
    </item>
    
    <item>
      <title>Getting Started with Regular Expressions</title>
      <link>/2020/03/27/getting-started-with-regular-expressions/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/27/getting-started-with-regular-expressions/</guid>
      <description>Regular expressions (or regex) are tools for matching patterns in character strings. These can be useful for finding words or letter patterns in text, parsing filenames for specific information, and interpreting input formatted in a variety of ways (e.g., phone numbers). The syntax of regular expressions is generally recognized across operating systems and programming languages. Although this tutorial will use R for examples, most of the same regex could be used in the unix commands or in python.</description>
    </item>
    
    <item>
      <title>Creating a SQLite database for use with R</title>
      <link>/2020/03/05/creating-a-sqlite-database-for-use-with-r/</link>
      <pubDate>Thu, 05 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/05/creating-a-sqlite-database-for-use-with-r/</guid>
      <description>When you import or load data into R, the data are stored in Random Access Memory (RAM). This is the memory that is deleted when you close R or shut off your computer. It’s very fast but temporary. If you save your data, it is saved to your hard drive. But when you open R again and load the data, once again it is loaded into RAM. While many newer computers come with lots of RAM (such as 16 GB), it’s not an infinite amount.</description>
    </item>
    
    <item>
      <title>Simulating Data for Count Models</title>
      <link>/2019/08/29/simulating-data-for-count-models/</link>
      <pubDate>Thu, 29 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/08/29/simulating-data-for-count-models/</guid>
      <description>A count model is a linear model where the dependent variable is a count. For example, the number of times a car breaks down, the number of rats in a litter, the number of times a young student gets out of his seat, etc. Counts are either 0 or a postive whole number, which means we need to use special distributions to generate the data.
The Poisson DistributionThe most common count distribution is the Poisson distribution.</description>
    </item>
    
    <item>
      <title>A Guide to Python in QGIS aka How to Help Yourself: Part 1</title>
      <link>/2019/06/03/a-guide-to-python-in-qgis-aka-how-to-help-yourself-part-1/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/03/a-guide-to-python-in-qgis-aka-how-to-help-yourself-part-1/</guid>
      <description>This post is something I’ve been thinking about writing for a while and will be several parts. I was inspired to write it by my own trials and tribulations, which are still ongoing, while working with the QGIS API, trying to programmatically do stuff in QGIS instead of relying on available widgets and plugins. I have spent, and will probably continue to spend, many hours scouring the internet and especially Stack Overflow looking for answers of how to use various classes, methods, attributes, etc.</description>
    </item>
    
    <item>
      <title>Reading PDF files into R for text mining</title>
      <link>/2019/05/14/reading-pdf-files-into-r-for-text-mining/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/14/reading-pdf-files-into-r-for-text-mining/</guid>
      <description>Let’s say we’re interested in text mining the opinions of The Supreme Court of the United States from the 2014 term. The opinions are published as PDF files at the following web page http://www.supremecourt.gov/opinions/slipopinion/14. We would probably want to look at all 76 opinions, but for the purposes of this introductory tutorial we’ll just look at the last three of the term: (1) Glossip v. Gross, (2) State Legislature v.</description>
    </item>
    
    <item>
      <title>Simulating a Logistic Regression Model</title>
      <link>/2019/05/04/simulating-a-logistic-regression-model/</link>
      <pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/04/simulating-a-logistic-regression-model/</guid>
      <description>Logistic regression is a method for modeling binary data as a function of other variables. For example we might want to model the occurrence or non-occurrence of a disease given predictors such as age, race, weight, etc. The result is a model that returns a predicted probability of occurrence (or non-occurrence, depending on how we set up our data) given certain values of our predictors. We might also be able to interpret the coefficients in our model to summarize how a change in one predictor affects the odds of occurrence.</description>
    </item>
    
    <item>
      <title>An Introduction to Analyzing Twitter Data with R</title>
      <link>/2019/05/03/an-introduction-to-analyzing-twitter-data-with-r/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/03/an-introduction-to-analyzing-twitter-data-with-r/</guid>
      <description>In this article, I will walk you through why a researcher or professional might find data from Twitter useful, explain how to collect the relevant tweets and information from Twitter in R, and then finish by demonstrating a few useful analyses (along with accompanying cleaning) you might perform on your Twitter data.
Part One: Why Twitter Data?To begin, we should ask: why would someone be interested in using data from Twitter?</description>
    </item>
    
    <item>
      <title>Getting Started with Multiple Imputation in R</title>
      <link>/2019/05/01/getting-started-with-multiple-imputation-in-r/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/01/getting-started-with-multiple-imputation-in-r/</guid>
      <description>Whenever we are dealing with a dataset, we almost always run into a problem that may decrease our confidence in the results that we are getting - missing data! Examples of missing data can be found in surveys - where respondents intentionally refrained from answering a question, didn’t answer a question because it is not applicable to them, or simply forgot to give an answer. Or our dataset on trade in agricultural products for country-pairs over years could suffer from missing data as some countries fail to report their accounts for certain years.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Wed, 17 Apr 2019 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>The UVA StatLab publishes short articles and tutorials on various statistical topics. We try our best to provide clear and accessible explanations. If you have questions, comments or requests, we want to hear from you: statlab@virginia.edu</description>
    </item>
    
    <item>
      <title>How to Create and Export Print Layouts in Python for QGIS 3</title>
      <link>/2019/02/15/how-to-create-and-export-print-layouts-in-python-for-qgis-3/</link>
      <pubDate>Fri, 15 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/02/15/how-to-create-and-export-print-layouts-in-python-for-qgis-3/</guid>
      <description>I’ve been struggling off and on for literally months trying to create and export a print layout using Python for QGIS 3. Or PyQGIS 3 for short. I have finally figured out may of the ins and outs of the process and hopefully this will serve as a guide to save someone else a lot of effort and time.
To begin I’ll point you to this link which has the full code, top to bottom.</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 5</title>
      <link>/2019/01/31/analysis-of-ours-to-shape-comments-part-5/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/01/31/analysis-of-ours-to-shape-comments-part-5/</guid>
      <description>IntroductionIn the penultimate post of this series, we’ll use some unsupervised learning approaches to uncover comment clusters and latent themes among the comments to President Ryan’s Ours to Shape website.
library(quanteda) # main text packagelibrary(tidyverse) # for dplyr, stringr, piping, etc.library(RColorBrewer) # better colors in graphslibrary(scales) # better breaks and labels in graphslibrary(stm) # structural topic modelsLet’s start with the document-feature matrix we created from the de-duplicated comments.</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 4</title>
      <link>/2018/12/19/analysis-of-ours-to-shape-comments-part-4/</link>
      <pubDate>Wed, 19 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/19/analysis-of-ours-to-shape-comments-part-4/</guid>
      <description>IntroductionIn the fourth installment of this series (we’re almost done, I promise), we’ll look at the sentiment – aka positive/negative tone, polarity, affect – of the comments to President Ryan’s Ours to Shape website.
We don’t have a pre-labeled set of comments, with negative or positive sentiment already identified, so we can’t use a supervised classification method (and I’m not committed enough to hand code a sample of comments).</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 3</title>
      <link>/2018/12/18/analysis-of-ours-to-shape-comments-part-3/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/18/analysis-of-ours-to-shape-comments-part-3/</guid>
      <description>IntroductionTo recap, we’re exploring the comments submitted to President Ryan’s Ours to Shape website (as of December 7, 2018).
In the first post we looked at the distribution of comments across Ryan’s three categories – community, discovery, and service – and across the contributors’ primary connection to the university. We extracted features like length and readability of the comments, and compared these across groups. And we explored the context in which key words of interest (to me) were used.</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 2</title>
      <link>/2018/12/14/analysis-of-ours-to-shape-comments-part-2/</link>
      <pubDate>Fri, 14 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/14/analysis-of-ours-to-shape-comments-part-2/</guid>
      <description>IntroductionIn the last post, we began exploring the Ours to Shape comments – the distribution across categories and contributors, the length and readability of the comments, and a few key words in context. While I did more exploration of the data than reported, the first post gives a taste of the kind of dive into the data that usefully proceeds analysis.
In this post, we’ll start digging into word frequencies, relative frequencies by groups, and distinguishing words.</description>
    </item>
    
    <item>
      <title>Analysis of Ours to Shape Comments, Part 1</title>
      <link>/2018/12/13/analysis-of-ours-to-shape-comments-part-1/</link>
      <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/13/analysis-of-ours-to-shape-comments-part-1/</guid>
      <description>IntroductionAs part of a series of workshops on quantitative analysis of text this fall, I started examining the comments submitted to President Ryan’s Ours to Shape website. The site invites people to share their ideas and insights for UVA going forward, particularly in the domains of service, discovery, and community. The website was only one venue for providing suggestions and voicing possibilities – President Ryan has hosted community discussions as well – but the website afforded an opportunity for individuals to chime in multiple times and at their convenience, so in theory should represent an especially inclusive collection.</description>
    </item>
    
    <item>
      <title>How to apply a graduated color symbology to a layer using Python for QGIS 3</title>
      <link>/2018/12/07/how-to-apply-a-graduated-color-symbology-to-a-layer-using-python-for-qgis-3/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/07/how-to-apply-a-graduated-color-symbology-to-a-layer-using-python-for-qgis-3/</guid>
      <description>I was recently working on a project in QGIS 3 with a member of UVA Health’s Oncologydepartment. This person wanted to take a set of patient data (after identifying info hadbeen removed) and after doing some other stuff, apply a graduated color scheme to theresults, shading them from light to dark based on intensity.
You can find a sample dataset for this project here:https://github.com/epurpur/PyQGIS-Scripts/blob/master/TestZipCodes.zip
This is a shapefile with a handful of zip codes in Virginia.</description>
    </item>
    
    <item>
      <title>How to use the field calculator in Python for QGIS 3</title>
      <link>/2018/12/07/how-to-use-the-field-calculator-in-python-for-qgis-3/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/07/how-to-use-the-field-calculator-in-python-for-qgis-3/</guid>
      <description>Recently, I have taken the dive into python scripting in QGIS. QGIS is a really niceopen source (and free!) alternative to ESRI’s ArcGIS. While QGIS is a little quirky andgenerally not quite as user friendly as ArcGIS, it still provides nearly the samefunctionality. Personally, I’ve become a fan of it an now have even taught a short,1 credit course in the University of Virginia’s Batten School of Public Policy titled:GIS for Public Policy.</description>
    </item>
    
    <item>
      <title>A Beginner’s Guide to Text Analysis with quanteda</title>
      <link>/2018/11/27/a-beginner-s-guide-to-text-analysis-with-quanteda/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/27/a-beginner-s-guide-to-text-analysis-with-quanteda/</guid>
      <description>A lot of introductory tutorials to quanteda assume that the reader has some base of knowledge about the program’s functionality or how it might be used. Other tutorials assume that the user is an expert in R and on what goes on under the hood when you’re coding. This introductory guide will assume none of that. Instead, I’m presuming a very basic understanding of R (like how to assign variables) and that you’ve just heard of quanteda for the first time today.</description>
    </item>
    
    <item>
      <title>Assessing Type S and Type M Errors</title>
      <link>/2018/10/31/assessing-type-s-and-type-m-errors/</link>
      <pubDate>Wed, 31 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/10/31/assessing-type-s-and-type-m-errors/</guid>
      <description>The paper Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors by Andrew Gelman and John Carlin introduces the idea of performing design calculations to help prevent researchers from being misled by statistically significant results in studies with small samples and/or noisy measurements. The main idea is that researchers often overestimate effect sizes in power calculations and collect noisy (ie, highly variable) data, which can make statistically significant results suspect.</description>
    </item>
    
    <item>
      <title>Interpreting Log Transformations in a Linear Model</title>
      <link>/2018/08/17/interpreting-log-transformations-in-a-linear-model/</link>
      <pubDate>Fri, 17 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/08/17/interpreting-log-transformations-in-a-linear-model/</guid>
      <description>Log transformations are often recommended for skewed data, such as monetary measures or certain biological and demographic measures. Log transforming data usually has the effect of spreading out clumps of data and bringing together spread-out data. For example, below is a histogram of the areas of all 50 US states. It is skewed to the right due to Alaska, California, Texas and a few others.
hist(state.area)After a log transformation, notice the histogram is more or less symmetric.</description>
    </item>
    
    <item>
      <title>Getting Started with Matching Methods</title>
      <link>/2018/04/24/getting-started-with-matching-methods/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/24/getting-started-with-matching-methods/</guid>
      <description>A frequent research question is whether or not some “treatment” causes an effect. For example, does taking aspirin daily reduce the chance of a heart attack? Does more sleep lead to better academic performance for teenagers? Does smoking increase the risk of chronic obstructive pulmonary disease (COPD)?
To truly answer such questions, we need a time machine and a lack of ethics. We would need to be able to take a subject and, say, make him or her smoke for several years and observe whether or not they get COPD.</description>
    </item>
    
    <item>
      <title>Getting Started with Moderated Mediation</title>
      <link>/2018/03/02/getting-started-with-moderated-mediation/</link>
      <pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/02/getting-started-with-moderated-mediation/</guid>
      <description>In a previous post we demonstrated how to perform a basic mediation analysis. In this post we look at performing a moderated mediation analysis.
The basic idea is that a mediator may depend on another variable called a “moderator”. For example, in our mediation analysis post we hypothesized that self-esteem was a mediator of student grades on the effect of student happiness. We illustrate this below with a path diagram.</description>
    </item>
    
    <item>
      <title>Getting started with Multivariate Multiple Regression</title>
      <link>/2017/10/27/getting-started-with-multivariate-multiple-regression/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/27/getting-started-with-multivariate-multiple-regression/</guid>
      <description>Multivariate Multiple Regression is the method of modeling multiple responses, or dependent variables, with a single set of predictor variables. For example, we might want to model both math and reading SAT scores as a function of gender, race, parent income, and so forth. This allows us to evaluate the relationship of, say, gender with each score. You may be thinking, “why not just run separate regressions for each dependent variable?</description>
    </item>
    
    <item>
      <title>Visualizing the Effects of Proportional-Odds Logistic Regression</title>
      <link>/2017/05/10/visualizing-the-effects-of-proportional-odds-logistic-regression/</link>
      <pubDate>Wed, 10 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/10/visualizing-the-effects-of-proportional-odds-logistic-regression/</guid>
      <description>Proportional-odds logistic regression is often used to model an ordered categorical response. By “ordered”, we mean categories that have a natural ordering, such as “Disagree”, “Neutral”, “Agree”, or “Everyday”, “Some days”, “Rarely”, “Never”. For a primer on proportional-odds logistic regression, see our post, Fitting and Interpreting a Proportional Odds Model. In this post we demonstrate how to visualize a proportional-odds model in R.
To begin, we load the effects package.</description>
    </item>
    
    <item>
      <title>Getting started with the purrr package in R</title>
      <link>/2017/04/14/getting-started-with-the-purrr-package-in-r/</link>
      <pubDate>Fri, 14 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/14/getting-started-with-the-purrr-package-in-r/</guid>
      <description>If you’re wondering what exactly the purrr package does, then this blog post is for you.
Before we get started, we should mention the Iteration chapter in R for Data Science by Garrett Grolemund and Hadley Wickham. We think this is the most thorough and extensive introduction to the purrr package currently available (at least at the time of this writing.) Wickham is one of the authors of the purrr package and he spends a good deal of the chapter clearly explaining how it works.</description>
    </item>
    
    <item>
      <title>Working with dates and time in R using the lubridate package</title>
      <link>/2017/01/11/working-with-dates-and-time-in-r-using-the-lubridate-package/</link>
      <pubDate>Wed, 11 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/11/working-with-dates-and-time-in-r-using-the-lubridate-package/</guid>
      <description>Sometimes we have data with dates and/or times that we want to manipulate or summarize. A common example in the health sciences is time-in-study. A subject may enter a study on Feb 12, 2008 and exit on November 4, 2009. How many days was the person in the study? (Don’t forget 2008 was a leap year; February had 29 days.) What was the median time-in-study for all subjects?
Another example are experiments that time participants performing an activity, applies a treatment to certain members, and then re-times the activity.</description>
    </item>
    
    <item>
      <title>The Wilcoxon Rank Sum Test</title>
      <link>/2017/01/05/the-wilcoxon-rank-sum-test/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/05/the-wilcoxon-rank-sum-test/</guid>
      <description>The Wilcoxon Rank Sum Test is often described as the non-parametric version of the two-sample t-test. You sometimes see it in analysis flowcharts after a question such as “is your data normal?” A “no” branch off this question will recommend a Wilcoxon test if you’re comparing two groups of continuous measures.
So what is this Wilcoxon test? What makes it non-parametric? What does that even mean? And how do we implement it and interpret it?</description>
    </item>
    
    <item>
      <title>Pairwise comparisons of proportions</title>
      <link>/2016/10/20/pairwise-comparisons-of-proportions/</link>
      <pubDate>Thu, 20 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/20/pairwise-comparisons-of-proportions/</guid>
      <description>Pairwise comparison means comparing all pairs of something. If I have three items A, B and C, that means comparing A to B, A to C, and B to C. Given n items, I can determine the number of possible pairs using the binomial coefficient:
\[ \frac{n!}{2!(n - 2)!} = \binom {n}{2}\]
Using the R statistical computing environment, we can use the choose function to quickly calculate this. For example, how many possible 2-item combinations can I “choose” from 10 items:</description>
    </item>
    
    <item>
      <title>Stata Basics: Combine Data (Append and Merge)</title>
      <link>/2016/10/14/stata-basics-combine-data-append-and-merge/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/14/stata-basics-combine-data-append-and-merge/</guid>
      <description>When I first started working with data, which was in a statistics class, we mostly used clean and completed dataset as examples. Later on, I realize it’s not always the case when doing research or data analysis for other purposes; in reality, we often need to put two or more dataset together to be able to begin whatever statistic analysis tasks we would like to perform. In this post, I demonstrate how to combine datasets into one file in two typical ways: append and merge, that are row-wise combining and column-wise combining, respectively.</description>
    </item>
    
    <item>
      <title>Stata Basics: Create, Recode and Label Variables</title>
      <link>/2016/10/14/stata-basics-create-recode-and-label-variables/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/14/stata-basics-create-recode-and-label-variables/</guid>
      <description>This post demonstrates how to create new variables, recode existing variables and label variables and values of variables. We use variables of the census.dta data that come with Stata as examples.
generate: create variablesHere we use the generate command to create a new variable representing populations younger than 18 years old. We do so by summing up the two existing variables: poplt5 (population &amp;lt; 5 years old) and pop5_17 (population of 5 to 17 years old).</description>
    </item>
    
    <item>
      <title>Stata Basics: Data Import, Use and Export</title>
      <link>/2016/10/14/stata-basics-data-import-use-and-export/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/14/stata-basics-data-import-use-and-export/</guid>
      <description>In Stata, the very first step of analyzing a dataset should be opening the dataset in Stata so that it knows which file you are going to work with. Yes, you can simply double click on a Stata data file that ends in .dta to open it, or you can do something fancier to achieve the same goal – like write some codes. Okay, there is at least one more reason than being fancier that makes me prefer to write syntax than clicking through things in Stata – I like to have everything I did recorded so that I can easily reproduce the same work or use the scripts again when working on similar tasks next time.</description>
    </item>
    
    <item>
      <title>Stata Basics: foreach and forvalues</title>
      <link>/2016/10/14/stata-basics-foreach-and-forvalues/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/14/stata-basics-foreach-and-forvalues/</guid>
      <description>There are times we need to do some repetitive tasks in the process of data preparation, analysis or presentation, for instance, computing a set of variables in a same manner, rename or create a series of variables, or repetitively recode values of a number of variables. In this post, I show a few of simple example “loops” using Stata commands foreach, local and forvalues to handle some common simple repetitive tasks.</description>
    </item>
    
    <item>
      <title>Stata Basics: Reshape Data</title>
      <link>/2016/10/14/stata-basics-reshape-data/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/14/stata-basics-reshape-data/</guid>
      <description>In this post, I use a few examples to illustrate the two common data forms: wide form and long form, and how to convert datasets between the two forms – here we call it “reshape” data. Reshaping is often needed when you work with datasets that contain variables with some kinds of sequences, say, time-series data. It is fairly easy to transform data between wide and long forms in Stata using the -reshape- command, however you’ll want to be careful when you convert a dataset from one form to another so that you can eliminate possible mistakes in the process of transforming.</description>
    </item>
    
    <item>
      <title>Stata Basics: Subset Data</title>
      <link>/2016/10/14/stata-basics-subset-data/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/14/stata-basics-subset-data/</guid>
      <description>Sometimes only parts of a dataset mean something to you. In this post, we show you how to subset a dataset in Stata, by variables or by observations. We use the census.dta dataset installed with Stata as the sample data.
* Load the data&amp;gt; sysuse census.dta(1980 Census data by state)* See the information of the data&amp;gt; describeContains data from /Applications/Stata/ado/base/c/census.dtaobs: 50 1980 Census data by statevars: 13 6 Apr 2014 15:43size: 2,900 ---------------------------------------------------------------------------storage display valuevariable name type format label variable label---------------------------------------------------------------------------state str14 %-14s Statestate2 str2 %-2s Two-letter state abbreviationregion int %-8.</description>
    </item>
    
    <item>
      <title>Getting UN Comtrade Data with R</title>
      <link>/2016/10/03/getting-un-comtrade-data-with-r/</link>
      <pubDate>Mon, 03 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/03/getting-un-comtrade-data-with-r/</guid>
      <description>The UN Comtrade Database provides free access to global trade data. You can get data by using their data extraction interface or API. In this post, we share some possible ways of downloading, preparing and plotting trade data in R.
Before running this script, you’ll need to install the rjson package if you haven’t done so before. Make sure your machine is connected to the Internet, and run install.packages(&#34;rjson”) - you only need to do this once.</description>
    </item>
    
    <item>
      <title>Using Data.gov APIs in R</title>
      <link>/2016/10/03/using-data-gov-apis-in-r/</link>
      <pubDate>Mon, 03 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/10/03/using-data-gov-apis-in-r/</guid>
      <description>Data.gov catalogs government data and makes them available on the web; you can find data in a variety of topics such as agriculture, business, climate, education, energy, finance, public safty and many more. It is a good start point for finding data if you don’t already know which particular data source to begin your search, however it can still be time consuming when it comes to actually downloading the raw data you need.</description>
    </item>
    
    <item>
      <title>Using Census Data API with R</title>
      <link>/2016/09/29/using-census-data-api-with-r/</link>
      <pubDate>Thu, 29 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/09/29/using-census-data-api-with-r/</guid>
      <description>Datasets provided by the US Census Bureau, such as Decennial Census and American Community Survey (ACS), are widely used by many researchers, among others. You can certainly find and download census data from the Census Bureau website, from our licensed data source Social Explorer, or other free sources such as IPUMS-USA, then load the data into one of the statistical packages or other softwares to analyze or present the data.</description>
    </item>
    
    <item>
      <title>A tidyr Tutorial</title>
      <link>/2016/08/24/a-tidyr-tutorial/</link>
      <pubDate>Wed, 24 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/24/a-tidyr-tutorial/</guid>
      <description>The tidyr package by Hadley Wickham centers on two functions: gather and spread. If you have struggled to understand what exactly these two functions do, this tutorial is for you.
To begin we need to wrap our heads around the idea of “key-value pairs”. The help pages for gather and spread use this terminology to explain how they work. Without some intuition for key-value pairs, it can be difficult to truly understand how these functions work.</description>
    </item>
    
    <item>
      <title>Getting Started with Factor Analysis</title>
      <link>/2016/08/01/getting-started-with-factor-analysis/</link>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/08/01/getting-started-with-factor-analysis/</guid>
      <description>Take a look at the following correlation matrix for Olympic decathlon data calculated from 280 scores from 1960 through 2004 (Johnson and Wichern, p. 499):
If we focus on the first row, 100m (100 meter dash), we see that it has fairly high correlations with LJ and 400m (long jump and 400 meter dash, 0.64 and 0.55) and somewhat lower correlations with all other events. Scoring high in the 100 meter dash seems to correlate with scoring high in the long jump and 400 meter run but gives no solid indication of performance in the other events.</description>
    </item>
    
    <item>
      <title>An Introduction to Loglinear Models</title>
      <link>/2016/07/12/an-introduction-to-loglinear-models/</link>
      <pubDate>Tue, 12 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/07/12/an-introduction-to-loglinear-models/</guid>
      <description>Loglinear models model cell counts in contingency tables. They’re a little different from other modeling methods in that they don’t distinguish between response and explanatory variables. All variables in a loglinear model are essentially “responses”.
To learn more about loglinear models, we’ll explore the following data from Agresti (1996, Table 6.3). It summarizes responses from a survey that asked high school seniors in a particular city whether they had ever used alcohol, cigarettes, or marijuana.</description>
    </item>
    
    <item>
      <title>Setting up Color Palettes in R</title>
      <link>/2016/06/10/setting-up-color-palettes-in-r/</link>
      <pubDate>Fri, 10 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/06/10/setting-up-color-palettes-in-r/</guid>
      <description>Plotting with color in R is kind of like painting a room in your house: you have to pick some colors. R has some default colors ready to go, but it’s only natural to want to play around and try some different combinations. In this post we’ll look at some ways you can define new color palettes for plotting in R.
To begin, let’s use the palette function to see what colors are currently available:</description>
    </item>
    
    <item>
      <title>Getting Started with Hurdle Models</title>
      <link>/2016/06/01/getting-started-with-hurdle-models/</link>
      <pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/06/01/getting-started-with-hurdle-models/</guid>
      <description>Hurdle Models are a class of models for count data that help handle excess zeros and overdispersion. To motivate their use, let’s look at some data in R.
The following data come with the AER package. It is a sample of 4,406 individuals, aged 66 and over, who were covered by Medicare in 1988. One of the variables the data provide is number of physician office visits. Let’s say we wish to model the number of vists (a count) by some of the other variables in the data set.</description>
    </item>
    
    <item>
      <title>Hierarchical Linear Regression</title>
      <link>/2016/05/20/hierarchical-linear-regression/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/05/20/hierarchical-linear-regression/</guid>
      <description>This post is NOT about Hierarchical Linear Modeling (HLM; multilevel modeling). The hierarchical regression is model comparison of nested regression models.
When do I want to perform hierarchical regression analysis?Hierarchical regression is a way to show if variables of your interest explain a statistically significant amount of variance in your Dependent Variable (DV) after accounting for all other variables. This is a framework for model comparison rather than a statistical method.</description>
    </item>
    
    <item>
      <title>Getting started with Negative Binomial Regression Modeling</title>
      <link>/2016/05/05/getting-started-with-negative-binomial-regression-modeling/</link>
      <pubDate>Thu, 05 May 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/05/05/getting-started-with-negative-binomial-regression-modeling/</guid>
      <description>When it comes to modeling counts (ie, whole numbers greater than or equal to 0), we often start with Poisson regression. This is a generalized linear model where a response is assumed to have a Poisson distribution conditional on a weighted sum of predictors. For example, we might model the number of documented concussions to NFL quarterbacks as a function of snaps played and the total years experience of his offensive line.</description>
    </item>
    
    <item>
      <title>Visualizing the Effects of Logistic Regression</title>
      <link>/2016/04/22/visualizing-the-effects-of-logistic-regression/</link>
      <pubDate>Fri, 22 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/04/22/visualizing-the-effects-of-logistic-regression/</guid>
      <description>Logistic regression is a popular and effective way of modeling a binary response. For example, we might wonder what influences a person to volunteer, or not volunteer, for psychological research. Some do, some don’t. Are there independent variables that would help explain or distinguish between those who volunteer and those who don’t? Logistic regression gives us a mathematical model that we can we use to estimate the probability of someone volunteering given certain independent variables.</description>
    </item>
    
    <item>
      <title>Introduction to Mediation Analysis</title>
      <link>/2016/04/18/introduction-to-mediation-analysis/</link>
      <pubDate>Mon, 18 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/04/18/introduction-to-mediation-analysis/</guid>
      <description>What is mediation?Let’s say previous studies have suggested that higher grades predict higher happiness: X (grades) \(\rightarrow\) Y (happiness). (This research example is made up for illustration purposes. Please don’t consider it a scientific statement.)
I think, however, grades are not the real reason that happiness increases. I hypothesize that good grades boost one’s self-esteem and then high self-esteem boosts one’s happiness: X (grades) \(\rightarrow\) M (self-esteem) \(\rightarrow\) Y (happiness).</description>
    </item>
    
    <item>
      <title>Understanding 2-way Interactions</title>
      <link>/2016/03/25/understanding-2-way-interactions/</link>
      <pubDate>Fri, 25 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/03/25/understanding-2-way-interactions/</guid>
      <description>When doing linear modeling or ANOVA it’s useful to examine whether or not the effect of one variable depends on the level of one or more variables. If it does then we have what is called an “interaction”. This means variables combine or interact to affect the response. The simplest type of interaction is the interaction between two two-level categorical variables. Let’s say we have gender (male and female), treatment (yes or no), and a continuous response measure.</description>
    </item>
    
    <item>
      <title>Comparing Proportions with Relative Risk and Odds Ratios</title>
      <link>/2016/01/08/comparing-proportions-with-relative-risk-and-odds-ratios/</link>
      <pubDate>Fri, 08 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/01/08/comparing-proportions-with-relative-risk-and-odds-ratios/</guid>
      <description>The classic two-by-two table displays counts of what may be called “successes” and “failures” versus some two-level grouping variable, such as gender (male and female) or treatment (placebo and active drug). An example of one such table is given in the book An Introduction to Categorical Data Analysis (Agresti, 1996, p. 20). The table classifies Myocardial Infarction (yes/no) with Group (Placebo/Aspirin). The data were “taken from a report on the relationship between aspirin use and myocardial infarction (heart attacks) by the Physicians’ Health Study Research Group at Harvard Medical School.</description>
    </item>
    
    <item>
      <title>Using and Interpreting Cronbach’s Alpha</title>
      <link>/2015/11/16/using-and-interpreting-cronbach-s-alpha/</link>
      <pubDate>Mon, 16 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/11/16/using-and-interpreting-cronbach-s-alpha/</guid>
      <description>I. What is Cronbach’s alpha?Cronbach’s alpha is a measure used to assess the reliability, or internal consistency, of a set of scale or test items. In other words, the reliability of any given measurement refers to the extent to which it is a consistent measure of a concept, and Cronbach’s alpha is one way of measuring the strength of that consistency.
Cronbach’s alpha is computed by correlating the score for each scale item with the total score for each observation (usually individual survey respondents or test takers), and then comparing that to the variance for all individual item scores:</description>
    </item>
    
    <item>
      <title>Is R-squared Useless?</title>
      <link>/2015/10/17/is-r-squared-useless/</link>
      <pubDate>Sat, 17 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/10/17/is-r-squared-useless/</guid>
      <description>On Thursday, October 16, 2015, a disbelieving student posted on Reddit My stats professor just went on a rant about how R-squared values are essentially useless, is there any truth to this? It attracted a fair amount of attention, at least compared to other posts about statistics on Reddit.
It turns out the student’s stats professor was Cosma Shalizi of Carnegie Mellon University. Shalizi provides free and open access to his class lecture materials so we can see what exactly he was “ranting” about.</description>
    </item>
    
    <item>
      <title>Fitting and Interpreting a Proportional Odds Model</title>
      <link>/2015/10/05/fitting-and-interpreting-a-proportional-odds-model/</link>
      <pubDate>Mon, 05 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/10/05/fitting-and-interpreting-a-proportional-odds-model/</guid>
      <description>Take a look at the following table. It is a cross tabulation of data taken from the 1991 General Social Survey that relates political party affiliation to political ideology. (Agresti, An Introduction to Categorical Data Analysis, 1996)
td{padding:0 15px 0 15px;text-align: center;}caption{font-size: 70%}Political Ideology by Party Affiliation, from the 1991 General Social SurveyVery Liberal</description>
    </item>
    
    <item>
      <title>Understanding Diagnostic Plots for Linear Regression Analysis</title>
      <link>/2015/09/21/understanding-diagnostic-plots-for-linear-regression-analysis/</link>
      <pubDate>Mon, 21 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/09/21/understanding-diagnostic-plots-for-linear-regression-analysis/</guid>
      <description>You ran a linear regression analysis and the stats software spit out a bunch of numbers. The results were significant (or not). You might think that you’re done with analysis. No, not yet. After running a regression analysis, you should check if the model works well for data.
We can check if a model works well for data in many different ways. We pay great attention to regression results, such as slope coefficients, p-values, or R2 that tell us how well a model represents given data.</description>
    </item>
    
    <item>
      <title>Getting Started with Quantile Regression</title>
      <link>/2015/09/20/getting-started-with-quantile-regression/</link>
      <pubDate>Sun, 20 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/09/20/getting-started-with-quantile-regression/</guid>
      <description>When we think of regression we usually think of linear regression, the tried and true method for estimating a mean of some variable conditional on the levels or values of independent variables. In other words, we’re pretty sure the mean of our variable of interest differs depending on other variables. For example the mean weight of 1st year UVa males is some unknown value. But we could in theory take a random sample and discover there is a relationship between weight and height.</description>
    </item>
    
    <item>
      <title>Should I always transform my variables to make them normal?</title>
      <link>/2015/09/14/should-i-always-transform-my-variables-to-make-them-normal/</link>
      <pubDate>Mon, 14 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/09/14/should-i-always-transform-my-variables-to-make-them-normal/</guid>
      <description>When I first learned data analysis, I always checked normality for each variable and made sure they were normally distributed before running any analyses, such as t-test, ANOVA, or linear regression. I thought normal distribution of variables was the important assumption to proceed to analyses. That’s why stats textbooks show you how to draw histograms and QQ-plots in the beginning of data analysis in the early chapters and see if they’re normally distributed, isn’t it?</description>
    </item>
    
    <item>
      <title>Simulating Endogeneity</title>
      <link>/2015/09/10/simulating-endogeneity/</link>
      <pubDate>Thu, 10 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/09/10/simulating-endogeneity/</guid>
      <description>First off, what is endogeneity, and why would we want to simulate it?
Endogeneity occurs when a statistical model has an independent variable that is correlated with the error term. The reason we would want to simulate it is to understand what exactly that definition means!
Let’s first simulate ideal data for simple linear regression using R.
# independent variablex &amp;lt;- seq(1,5,length.out = 100) # error (uncorrelated with x)set.</description>
    </item>
    
    <item>
      <title>Understanding Q-Q Plots</title>
      <link>/2015/08/26/understanding-q-q-plots/</link>
      <pubDate>Wed, 26 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/08/26/understanding-q-q-plots/</guid>
      <description>The Q-Q plot, or quantile-quantile plot, is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a Normal or exponential. For example, if we run a statistical analysis that assumes our dependent variable is Normally distributed, we can use a Normal Q-Q plot to check that assumption. It’s just a visual check, not an air-tight proof, so it is somewhat subjective.</description>
    </item>
    
  </channel>
</rss>